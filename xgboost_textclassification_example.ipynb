{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "# example - https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n",
    "# https://github.com/ameasure/autocoding-class/blob/master/machine_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv file into a DataFrame\n",
    "df = pd.read_csv(r'Health_Care_Ticket.csv', encoding='iso-8859-1')\n",
    "#supplemental = pd.read_csv(r'jobboard_reed_uk_secondary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57280\n",
      "53932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>SUMMARY</th>\n",
       "      <th>DATA</th>\n",
       "      <th>categories</th>\n",
       "      <th>sub_categories</th>\n",
       "      <th>previous_appointment</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015561331001</td>\n",
       "      <td>Pt aware that he needs ROV for refill</td>\n",
       "      <td>{\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...</td>\n",
       "      <td>PRESCRIPTION</td>\n",
       "      <td>REFILL</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6133_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015561341001</td>\n",
       "      <td>Mom wants to know if the Focalin needs some do...</td>\n",
       "      <td>{\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...</td>\n",
       "      <td>ASK_A_DOCTOR</td>\n",
       "      <td>MEDICATION RELATED</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6134_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015561351001</td>\n",
       "      <td>pt called to discuss nortryptiline. she says s...</td>\n",
       "      <td>xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...</td>\n",
       "      <td>ASK_A_DOCTOR</td>\n",
       "      <td>MEDICATION RELATED</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6135_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015561361001</td>\n",
       "      <td>FYI Nortryptline medication.</td>\n",
       "      <td>xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6136_1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015561371001</td>\n",
       "      <td>Letter of patient establishment request</td>\n",
       "      <td>{\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)</td>\n",
       "      <td>No</td>\n",
       "      <td>2015_5_6137_1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fileid                                            SUMMARY  \\\n",
       "0  2015561331001              Pt aware that he needs ROV for refill   \n",
       "1  2015561341001  Mom wants to know if the Focalin needs some do...   \n",
       "2  2015561351001  pt called to discuss nortryptiline. she says s...   \n",
       "3  2015561361001                       FYI Nortryptline medication.   \n",
       "4  2015561371001            Letter of patient establishment request   \n",
       "\n",
       "                                                DATA     categories  \\\n",
       "0  {\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...   PRESCRIPTION   \n",
       "1  {\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...   ASK_A_DOCTOR   \n",
       "2  xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...   ASK_A_DOCTOR   \n",
       "3  xxxx-xxxx\\f0 \\fswiss Arial;}}{\\colortbl ;\\red2...  MISCELLANEOUS   \n",
       "4  {\\rtf1\\ansi\\ftnbj{\\fonttbl{\\f0 \\fswiss Arial;}...  MISCELLANEOUS   \n",
       "\n",
       "                                  sub_categories previous_appointment  \\\n",
       "0                                         REFILL                   No   \n",
       "1                             MEDICATION RELATED                   No   \n",
       "2                             MEDICATION RELATED                   No   \n",
       "3                                         OTHERS                   No   \n",
       "4  SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)                   No   \n",
       "\n",
       "                 ID  \n",
       "0  2015_5_6133_1001  \n",
       "1  2015_5_6134_1001  \n",
       "2  2015_5_6135_1001  \n",
       "3  2015_5_6136_1001  \n",
       "4  2015_5_6137_1001  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut down the training data to 10K for manageability\n",
    "print(df.shape[0])\n",
    "df = df.dropna()\n",
    "print(df.shape[0])\n",
    "df = df.iloc[0:10000,:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data in train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  6000\n",
      "validation dataset:  2000\n",
      "testing dataset:  2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_categories</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>REFILL</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MEDICATION RELATED</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NEW APPOINTMENT</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OTHERS</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sub_categories  labels\n",
       "15                                         REFILL    1612\n",
       "7                              MEDICATION RELATED    1449\n",
       "8                                 NEW APPOINTMENT    1325\n",
       "9                                          OTHERS     633\n",
       "18  SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)     562"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first lets stratify the train (+valid) and test data, such that the test data is reflective of the classes we might see\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_valid, test = train_test_split(df, test_size=0.2, stratify=df[\"sub_categories\"], random_state=42) \n",
    "train, valid = train_test_split(train_valid, test_size=0.25, stratify=train_valid[\"sub_categories\"], random_state=42) \n",
    "\n",
    "#train_valid, test = train_test_split(df, test_size=0.2, random_state=42) \n",
    "#train, valid = train_test_split(train_valid, test_size=0.25, random_state=42) \n",
    "\n",
    "print(\"training dataset: \", train.shape[0])\n",
    "print(\"validation dataset: \", valid.shape[0])\n",
    "print(\"testing dataset: \", test.shape[0])\n",
    "#print(\"supplemental dataset: \", supplemental.shape[0])\n",
    "\n",
    "# display class sizes of train_valid split\n",
    "df_group = train_valid.groupby(['sub_categories']).size().reset_index(name='labels').sort_values(by=['labels'], ascending=False)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = train.copy()\n",
    "df_train = train.iloc[0:1000,:]\n",
    "df_test = test.copy()\n",
    "df_valid = valid.copy()\n",
    "\n",
    "# rename classes and labels\n",
    "df_train = df_train.rename(columns={'SUMMARY': 'text', 'sub_categories':'code'})\n",
    "df_test = df_test.rename(columns={'SUMMARY': 'text', 'sub_categories':'code'})\n",
    "df_valid = df_valid.rename(columns={'SUMMARY': 'text', 'sub_categories':'code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the error rate we want to predict at we find what our autocoding rate will be lowered to\n",
    "def threshold_byerror(error_tolerance, code, pred, prob):\n",
    "    df = pd.DataFrame({'code': code, 'pred': pred, 'prob': prob})\n",
    "    \n",
    "    # create a threshold report\n",
    "    range = np.arange(0.5, 1.0, 0.01).tolist()\n",
    "    range.sort(reverse=True)\n",
    "    df_scores = pd.DataFrame(range, columns=['confidence'])\n",
    "\n",
    "    # find the threshold \n",
    "    def threshold_error(data, pred, prob, value):\n",
    "        df_temp = data[data[prob] > value]\n",
    "        accuracy = round(accuracy_score(df_temp['code'], df_temp[pred]) * 100, 2)\n",
    "        error_rate = 100 - accuracy\n",
    "        return error_rate\n",
    "\n",
    "    def threshold_percent(data, prob, value):\n",
    "        count_overall = data.shape[0]\n",
    "        df_temp = data[data[prob] > value]\n",
    "        count_threshold = df_temp.shape[0]\n",
    "        percent = round((count_threshold / count_overall) * 100, 2)\n",
    "        return percent\n",
    "    \n",
    "    df_scores['error'] =  df_scores.apply(lambda row: threshold_error(df, 'pred', 'prob', row['confidence']), axis=1)\n",
    "    df_scores['rate'] =  df_scores.apply(lambda row: threshold_percent(df, 'prob', row['confidence']), axis=1)\n",
    "    df_scores = df_scores.replace(np.nan,0)\n",
    "    \n",
    "    df_selected = df_scores.iloc[(df_scores['error'] - error_tolerance).abs().argsort()[:2]]\n",
    "    threshold_selected = round(df_selected.confidence.iloc[0]*100,2)\n",
    "    error_selected = round(df_selected.error.iloc[0],2)\n",
    "    rate_selected = round(df_selected.rate.iloc[0],2)\n",
    "    #text = (\"Threshold: \" + str(threshold_selected) +\n",
    "    #       \"%. Error: \" + str(error_selected) + \"%. Autocoding: \" + str(rate_selected) + \"%.\")\n",
    "    return threshold_selected, error_selected, rate_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23 @10 10:25 is a date\n",
      "date @10 date is a date\n"
     ]
    }
   ],
   "source": [
    "# source: https://stackoverflow.com/questions/37473219/how-to-remove-dates-from-a-list-in-python\n",
    "# source: https://stackoverflow.com/questions/26294245/python-regex-find-all-matches-in-string-and-replace\n",
    "import re\n",
    "\n",
    "def replace_date(text):\n",
    "    # order matters\n",
    "    if re.search(r'[0-9]{2}[\\/,:][0-9]{2}[\\/,:][0-9]{2,4}', text, re.IGNORECASE):\n",
    "        r = re.compile(r'[0-9]{2}[\\/,:][0-9]{2}[\\/,:][0-9]{2,4}', re.IGNORECASE)\n",
    "        text = r.sub(r'date', text)\n",
    "    if re.search(r'[0-9]{1,2}[\\/,:][0-9]{2,4}', text, re.IGNORECASE):\n",
    "        r = re.compile(r'[0-9]{1,2}[\\/,:][0-9]{2,4}', re.IGNORECASE)\n",
    "        text = r.sub(r'date', text)\n",
    "    if re.search(r'[0-9]{4}', text, re.IGNORECASE):\n",
    "        r = re.compile(r'[0-9]{4}', re.IGNORECASE)\n",
    "        text = r.sub(r'date', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "string = \"11/23 @10 10:25 is a date\"\n",
    "print(string)\n",
    "print(replace_date(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-3/\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def pipelinize(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series, active=True):\n",
    "        if active:\n",
    "            return [function(i) for i in list_or_series]\n",
    "        else: # if it's not active, just pass it right back\n",
    "            return list_or_series\n",
    "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text cleaning \n",
    "import re\n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(\"\\'\", \"\", text)           # remove backslash-apostrophe\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)     # remove everything except alphabets \n",
    "    text = ' '.join(text.split())           # remove whitespaces \n",
    "    text = text.lower()                     # convert text to lowercase \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "vect = CountVectorizer(strip_accents = ascii,\n",
    "                       lowercase = False,\n",
    "                       analyzer = 'word',\n",
    "                       binary=True) # one-hot encoding (true)\n",
    "\n",
    "\n",
    "# resource: https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\n",
    "\n",
    "# https://stackoverflow.com/questions/36253258/how-to-fit-different-inputs-into-an-sklearn-pipeline\n",
    "# TfidfTransformer and POSTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight vectors using tf-idf\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "                    eta = 0.2, # learning rate\n",
    "                    nthread = -1,\n",
    "                    seed = 42,\n",
    "                    \n",
    "                    # values to tune\n",
    "                    max_depth=5, # higher depth allow model to learn relations very specific to a particular sample (3-10)\n",
    "                    gamma = 0, # specifies the minimum loss reduction required to make a split\n",
    "                    scale_pos_weight = 3, # >0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "                    subsample = 0.5, # (0.5-1) lower to help overfitting, fraction of obs randomly samples for each tree\n",
    "                    min_child_weight=1, # higher value helps overfitting (min “number of observations”)\n",
    "                    \n",
    "                    # overfitting adjustment\n",
    "                    alpha=0, # L1 regularization term on weight\n",
    "                    lamda=1 # L2 regularization term on weights\n",
    "                    \n",
    "                    ) # set to use all\n",
    "\n",
    "# resource: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline(steps=[ ('replace_date',pipelinize(replace_date)),\n",
    "                            ('clean',pipelinize(clean_text)),\n",
    "                            ('vect', vect),\n",
    "                            ('tfidf', tfidf),\n",
    "                            ('clf', xgb),\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text_clf.fit(df_train.text, df_train.code) # create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin Evans\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "      <th>threshold_selected</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>autocoding_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.797270</td>\n",
       "      <td>0.813766</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.71</td>\n",
       "      <td>70.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valid</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.486058</td>\n",
       "      <td>0.491418</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>86.0</td>\n",
       "      <td>10.05</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  accuracy  weighted_f1  weighted_precision  weighted_recall  \\\n",
       "0   train    0.8020     0.797270            0.813766           0.8020   \n",
       "1   valid    0.5005     0.486058            0.491418           0.5005   \n",
       "\n",
       "   threshold_selected  error_rate  autocoding_rate  \n",
       "0                50.0        5.71             70.1  \n",
       "1                86.0       10.05             19.4  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "y_train = df_train.code\n",
    "y_train_pred = model.predict(df_train.text)\n",
    "y_train_prob = model.predict_proba(df_train.text) # gives an array of all class probabilities\n",
    "y_train_prob_max = y_train_prob.max(axis=1) # find the highest prob, associated with clf.predict()\n",
    "\n",
    "y_valid = df_valid.code\n",
    "y_valid_pred = model.predict(df_valid.text)\n",
    "y_valid_prob = model.predict_proba(df_valid.text) # gives an array of all class probabilities\n",
    "y_valid_prob_max = y_valid_prob.max(axis=1) # find the highest prob, associated with clf.predict()\n",
    "\n",
    "\n",
    "# metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_w_f1 = f1_score(y_train, y_train_pred, average = \"weighted\")\n",
    "train_w_precision = precision_score(y_train, y_train_pred, average = \"weighted\")\n",
    "train_w_recall = recall_score(y_train, y_train_pred, average = \"weighted\") \n",
    "train_threshold, train_error, train_rate = threshold_byerror(10,y_train,y_train_pred,y_train_prob_max)\n",
    "\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "valid_w_f1 = f1_score(y_valid, y_valid_pred, average = \"weighted\")\n",
    "valid_w_precision = precision_score(y_valid, y_valid_pred, average = \"weighted\")\n",
    "valid_w_recall = recall_score(y_valid, y_valid_pred, average = \"weighted\")    \n",
    "valid_threshold, valid_error, valid_rate = threshold_byerror(10,y_valid,y_valid_pred,y_valid_prob_max)\n",
    "\n",
    "# create a dataframe from the logged lists\n",
    "df_train_valid_results = pd.DataFrame(\n",
    "    {'dataset': ['train','valid'],\n",
    "     'accuracy': [train_accuracy, valid_accuracy],\n",
    "      'weighted_f1': [train_w_f1,valid_w_f1],\n",
    "      'weighted_precision': [train_w_precision, valid_w_precision],\n",
    "      'weighted_recall': [train_w_recall, valid_w_recall],\n",
    "      'threshold_selected':[train_threshold,valid_threshold],\n",
    "      'error_rate':[train_error, valid_error],\n",
    "     'autocoding_rate':[train_rate, valid_rate]\n",
    "        \n",
    "     })\n",
    "df_train_valid_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Using RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "                'vect__analyzer':('word', 'char', 'char_wb'),\n",
    "                'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__max_depth': range(3, 10),\n",
    "                'clf__gamma':(0,1,5,10),\n",
    "                'clf__min_child_weight':(1,5,10),\n",
    "                'clf__scale_pos_weight': range(1,5),\n",
    "                'clf__subsample': (0.5,0.75,1)        \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin Evans\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed:   45.2s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   55.7s finished\n"
     ]
    }
   ],
   "source": [
    "# n_iters = iterations, cv = folds\n",
    "rs_clf = RandomizedSearchCV(text_clf, parameters, n_iter=20, cv = 3, \n",
    "                            verbose=10,\n",
    "                            n_jobs=-1,\n",
    "                            random_state = 42)\n",
    "rs_clf = rs_clf.fit(df_train.text, df_train.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>param_vect__analyzer</th>\n",
       "      <th>param_clf__subsample</th>\n",
       "      <th>param_clf__scale_pos_weight</th>\n",
       "      <th>param_clf__min_child_weight</th>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <th>param_clf__gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19.284107</td>\n",
       "      <td>0.825339</td>\n",
       "      <td>0.049586</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'vect__analyzer'...</td>\n",
       "      <td>0.479042</td>\n",
       "      <td>0.495495</td>\n",
       "      <td>0.450450</td>\n",
       "      <td>0.474996</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.085725</td>\n",
       "      <td>0.051870</td>\n",
       "      <td>0.073142</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'vect__analyzer'...</td>\n",
       "      <td>0.476048</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.420420</td>\n",
       "      <td>0.460985</td>\n",
       "      <td>0.028998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.843483</td>\n",
       "      <td>0.067421</td>\n",
       "      <td>0.081121</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'vect__analyzer'...</td>\n",
       "      <td>0.458084</td>\n",
       "      <td>0.471471</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.456999</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.091089</td>\n",
       "      <td>0.548493</td>\n",
       "      <td>0.060845</td>\n",
       "      <td>0.011191</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'vect__analyzer'...</td>\n",
       "      <td>0.488024</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.423423</td>\n",
       "      <td>0.456969</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.278071</td>\n",
       "      <td>0.618233</td>\n",
       "      <td>0.050368</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>word</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>{'vect__ngram_range': (1, 2), 'vect__analyzer'...</td>\n",
       "      <td>0.434132</td>\n",
       "      <td>0.387387</td>\n",
       "      <td>0.423423</td>\n",
       "      <td>0.414981</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "17      19.284107      0.825339         0.049586        0.007295   \n",
       "13       8.085725      0.051870         0.073142        0.001427   \n",
       "15       6.843483      0.067421         0.081121        0.003507   \n",
       "19      11.091089      0.548493         0.060845        0.011191   \n",
       "3       16.278071      0.618233         0.050368        0.002479   \n",
       "\n",
       "   param_vect__ngram_range param_vect__analyzer param_clf__subsample  \\\n",
       "17                  (1, 2)                 char                    1   \n",
       "13                  (1, 2)                 char                  0.5   \n",
       "15                  (1, 2)              char_wb                 0.75   \n",
       "19                  (1, 2)                 char                  0.5   \n",
       "3                   (1, 2)                 word                 0.75   \n",
       "\n",
       "   param_clf__scale_pos_weight param_clf__min_child_weight  \\\n",
       "17                           1                           1   \n",
       "13                           3                           5   \n",
       "15                           2                          10   \n",
       "19                           4                           1   \n",
       "3                            1                           1   \n",
       "\n",
       "   param_clf__max_depth param_clf__gamma  \\\n",
       "17                    7                1   \n",
       "13                    9                1   \n",
       "15                    3                1   \n",
       "19                    7                1   \n",
       "3                     6               10   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "17  {'vect__ngram_range': (1, 2), 'vect__analyzer'...           0.479042   \n",
       "13  {'vect__ngram_range': (1, 2), 'vect__analyzer'...           0.476048   \n",
       "15  {'vect__ngram_range': (1, 2), 'vect__analyzer'...           0.458084   \n",
       "19  {'vect__ngram_range': (1, 2), 'vect__analyzer'...           0.488024   \n",
       "3   {'vect__ngram_range': (1, 2), 'vect__analyzer'...           0.434132   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "17           0.495495           0.450450         0.474996        0.018611   \n",
       "13           0.486486           0.420420         0.460985        0.028998   \n",
       "15           0.471471           0.441441         0.456999        0.012284   \n",
       "19           0.459459           0.423423         0.456969        0.026432   \n",
       "3            0.387387           0.423423         0.414981        0.019995   \n",
       "\n",
       "    rank_test_score  \n",
       "17                1  \n",
       "13                2  \n",
       "15                3  \n",
       "19                4  \n",
       "3                 5  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomsearch = pd.DataFrame(rs_clf.cv_results_)\n",
    "randomsearch = randomsearch.sort_values(by=['rank_test_score'], ascending=True)\n",
    "randomsearch.head()\n",
    "#randomsearch.to_csv(r'xgb_random_search.csv')\n",
    "\n",
    "#rs_clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune Using Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, verbose=10) # verbose adds details during logging, n_jobs(-1) = in parallel\n",
    "gs_clf = gs_clf.fit(df_train.text, df_train.code)\n",
    "\n",
    "# results of gridsearch\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)\n",
    "gridsearch = pd.DataFrame(gs_clf.cv_results_)\n",
    "gridsearch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is very long, a bayesian tuning or random search would be nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model (Train with Entire Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train.copy()\n",
    "df_train = df_train.rename(columns={'SUMMARY': 'text', 'sub_categories':'code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "vect = CountVectorizer(strip_accents = ascii,\n",
    "                       lowercase = True,\n",
    "                       ngram_range = (1,2),\n",
    "                       analyzer = 'char',\n",
    "                       \n",
    "                       binary=True) # one-hot encoding (true)\n",
    "\n",
    "xgb = XGBClassifier(eta = 0.2, \n",
    "                    nthread = -1,\n",
    "                    seed = 42,\n",
    "                    max_depth=7,\n",
    "                    gamma = 3,\n",
    "                    scale_pos_weight = 1,\n",
    "                    subsample = 0.75,\n",
    "                    min_child_weight=1,\n",
    "                    alpha=0, \n",
    "                    lamda=1)\n",
    "\n",
    "text_clf = Pipeline(steps=[('replace_date',pipelinize(replace_date)),\n",
    "                            ('clean',pipelinize(clean_text)),\n",
    "                            ('vect', vect),\n",
    "                            ('tfidf', tfidf),\n",
    "                            ('clf', xgb),])\n",
    "\n",
    "model = text_clf.fit(df_train.text, df_train.code) # create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin Evans\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Justin Evans\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "C:\\Users\\Justin Evans\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "      <th>threshold_selected</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>autocoding_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>0.835724</td>\n",
       "      <td>0.851122</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>68.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.549188</td>\n",
       "      <td>0.555195</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>79.0</td>\n",
       "      <td>10.03</td>\n",
       "      <td>29.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  accuracy  weighted_f1  weighted_precision  weighted_recall  \\\n",
       "0   train  0.835333     0.835724            0.851122         0.835333   \n",
       "1    test  0.569500     0.549188            0.555195         0.569500   \n",
       "\n",
       "   threshold_selected  error_rate  autocoding_rate  \n",
       "0                50.0        5.12            68.95  \n",
       "1                79.0       10.03            29.40  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "y_train = df_train.code\n",
    "y_train_pred = model.predict(df_train.text)\n",
    "y_train_prob = model.predict_proba(df_train.text) # gives an array of all class probabilities\n",
    "y_train_prob_max = y_train_prob.max(axis=1) # find the highest prob, associated with clf.predict()\n",
    "\n",
    "y_test = df_test.code\n",
    "y_test_pred = model.predict(df_test.text)\n",
    "y_test_prob = model.predict_proba(df_test.text) # gives an array of all class probabilities\n",
    "y_test_prob_max = y_test_prob.max(axis=1) # find the highest prob, associated with clf.predict()\n",
    "\n",
    "\n",
    "# metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_w_f1 = f1_score(y_train, y_train_pred, average = \"weighted\")\n",
    "train_w_precision = precision_score(y_train, y_train_pred, average = \"weighted\")\n",
    "train_w_recall = recall_score(y_train, y_train_pred, average = \"weighted\") \n",
    "train_threshold, train_error, train_rate = threshold_byerror(10,y_train,y_train_pred,y_train_prob_max)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_w_f1 = f1_score(y_test, y_test_pred, average = \"weighted\")\n",
    "test_w_precision = precision_score(y_test, y_test_pred, average = \"weighted\")\n",
    "test_w_recall = recall_score(y_test, y_test_pred, average = \"weighted\")    \n",
    "test_threshold, test_error, test_rate = threshold_byerror(10,y_test,y_test_pred,y_test_prob_max)\n",
    "\n",
    "# create a dataframe from the logged lists\n",
    "df_train_test_results = pd.DataFrame(\n",
    "    {'dataset': ['train','test'],\n",
    "     'accuracy': [train_accuracy, test_accuracy],\n",
    "      'weighted_f1': [train_w_f1,test_w_f1],\n",
    "      'weighted_precision': [train_w_precision, test_w_precision],\n",
    "      'weighted_recall': [train_w_recall, test_w_recall],\n",
    "      'threshold_selected':[train_threshold,test_threshold],\n",
    "      'error_rate':[train_error, test_error],\n",
    "     'autocoding_rate':[train_rate, test_rate]\n",
    "        \n",
    "     })\n",
    "df_train_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
